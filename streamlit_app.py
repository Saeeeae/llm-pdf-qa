import streamlit as st
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import io
import os
import requests
import json

# ë°±ì—”ë“œ ì„¤ì • (vllm ë˜ëŠ” ollama)
BACKEND = os.getenv("BACKEND", "vllm")
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-20B-Instruct" if BACKEND == "vllm" else "qwen2.5:14b")

def extract_text_from_pdf(pdf_file):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    pdf_reader = PdfReader(io.BytesIO(pdf_file.read()))
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    chunks = text_splitter.split_text(text)
    return chunks

def check_server_health():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        if BACKEND == "vllm":
            base_url = os.getenv("VLLM_API_BASE", "http://localhost:8000/v1")
            url = base_url.replace("/v1", "/health")
        else:
            base_url = os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
            url = f"{base_url}/api/tags"

        response = requests.get(url, timeout=5)
        return response.status_code == 200
    except:
        return False

def generate_response_ollama(prompt, context_chunks, model=None):
    """Ollamaë¥¼ í†µí•´ ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)"""
    if model is None:
        model = MODEL_NAME

    # ì»¨í…ìŠ¤íŠ¸ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•©
    context_text = "\n\n".join([f"[Document Part {i+1}]\n{chunk}" for i, chunk in enumerate(context_chunks)])

    system_message = "ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    user_message = f"ë¬¸ì„œ ë‚´ìš©:\n\n{context_text}\n\nì§ˆë¬¸: {prompt}"

    full_prompt = f"{system_message}\n\n{user_message}"

    ollama_base_url = os.getenv("OLLAMA_API_BASE", "http://localhost:11434")

    payload = {
        "model": model,
        "prompt": full_prompt,
        "stream": True,
        "options": {
            "temperature": 0.7,
            "num_predict": 2000
        }
    }

    try:
        response = requests.post(
            f"{ollama_base_url}/api/generate",
            json=payload,
            stream=True,
            timeout=300
        )
        response.raise_for_status()
        return response
    except Exception as e:
        st.error(f"Ollama API ì˜¤ë¥˜: {str(e)}")
        return None

def generate_response_vllm(prompt, context_chunks, model=None):
    """vLLMì„ í†µí•´ ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°) - HTTP ì§ì ‘ ì‚¬ìš©"""
    if model is None:
        model = MODEL_NAME

    # ì»¨í…ìŠ¤íŠ¸ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•©
    context_text = "\n\n".join([f"[Document Part {i+1}]\n{chunk}" for i, chunk in enumerate(context_chunks)])

    system_message = "ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    user_message = f"ë¬¸ì„œ ë‚´ìš©:\n\n{context_text}\n\nì§ˆë¬¸: {prompt}"

    vllm_base_url = os.getenv("VLLM_API_BASE", "http://localhost:8000/v1")

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ],
        "stream": True,
        "max_tokens": 2000,
        "temperature": 0.7
    }

    try:
        response = requests.post(
            f"{vllm_base_url}/chat/completions",
            json=payload,
            headers={"Content-Type": "application/json"},
            stream=True,
            timeout=300
        )
        response.raise_for_status()
        return response
    except Exception as e:
        st.error(f"vLLM API ì˜¤ë¥˜: {str(e)}")
        return None

# Streamlit UI
st.set_page_config(page_title="PDF Q&A with LLM", page_icon="ğŸ“„", layout="wide")

# ë°±ì—”ë“œ í‘œì‹œ
backend_emoji = "ğŸš€" if BACKEND == "vllm" else "ğŸ"
backend_name = "vLLM (GPU)" if BACKEND == "vllm" else "Ollama (CPU)"

st.title(f"ğŸ“„ PDF ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ {backend_emoji}")
st.markdown(f"**{backend_name} + {MODEL_NAME}**")

# ì„œë²„ ìƒíƒœ í™•ì¸
server_status = check_server_health()
if server_status:
    st.success(f"âœ… {backend_name} ì„œë²„ ì—°ê²°ë¨")
else:
    st.error(f"âŒ {backend_name} ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")

# ì‚¬ì´ë“œë°”: PDF ì—…ë¡œë“œ ë° ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # ì‹œìŠ¤í…œ ì •ë³´
    with st.expander("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´"):
        st.write(f"**ë°±ì—”ë“œ**: {backend_name}")
        st.write(f"**ëª¨ë¸**: {MODEL_NAME}")
        st.write(f"**ì„œë²„ ìƒíƒœ**: {'ğŸŸ¢ ì˜¨ë¼ì¸' if server_status else 'ğŸ”´ ì˜¤í”„ë¼ì¸'}")

    st.divider()

    uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

    st.divider()

    chunk_size = st.slider("ì²­í¬ í¬ê¸°", min_value=500, max_value=2000, value=1000, step=100)
    chunk_overlap = st.slider("ì²­í¬ ì˜¤ë²„ë©", min_value=0, max_value=500, value=200, step=50)

    if uploaded_file:
        with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
            # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            pdf_text = extract_text_from_pdf(uploaded_file)

            # ì²­í¬ë¡œ ë¶„í• 
            text_chunks = split_text_into_chunks(pdf_text, chunk_size, chunk_overlap)

            # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥
            st.session_state.pdf_text = pdf_text
            st.session_state.text_chunks = text_chunks

            st.success(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ!")
            st.info(f"ì´ {len(text_chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")

            # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
            with st.expander("í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°"):
                st.text_area("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 1000ì)", pdf_text[:1000], height=200)

# ë©”ì¸ ì˜ì—­: ì§ˆë¬¸ ë° ì‘ë‹µ
if "text_chunks" in st.session_state:
    st.header("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")

    # í”„ë¡¬í”„íŠ¸ ì…ë ¥
    user_prompt = st.text_area(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
        height=100
    )

    col1, col2 = st.columns([1, 5])
    with col1:
        submit_button = st.button("ì§ˆë¬¸í•˜ê¸°", type="primary", use_container_width=True, disabled=not server_status)
    with col2:
        if st.button("ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            if "chat_history" in st.session_state:
                st.session_state.chat_history = []
                st.rerun()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ì§ˆë¬¸ ì œì¶œ
    if submit_button and user_prompt:
        if not server_status:
            st.error("ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.chat_history.append({"role": "user", "content": user_prompt})

            # ì‘ë‹µ ìƒì„±
            with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                try:
                    response_placeholder = st.empty()
                    full_response = ""

                    if BACKEND == "vllm":
                        # vLLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                        stream = generate_response_vllm(user_prompt, st.session_state.text_chunks)
                        if stream:
                            for line in stream.iter_lines():
                                if line:
                                    line_str = line.decode('utf-8')
                                    if line_str.startswith("data: "):
                                        line_str = line_str[6:]  # "data: " ì œê±°
                                    if line_str.strip() == "[DONE]":
                                        break
                                    try:
                                        json_response = json.loads(line_str)
                                        if "choices" in json_response and len(json_response["choices"]) > 0:
                                            delta = json_response["choices"][0].get("delta", {})
                                            content = delta.get("content", "")
                                            if content:
                                                full_response += content
                                                response_placeholder.markdown(f"**AI:** {full_response}â–Œ")
                                    except json.JSONDecodeError:
                                        continue
                    else:
                        # Ollama ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                        stream = generate_response_ollama(user_prompt, st.session_state.text_chunks)
                        if stream:
                            for line in stream.iter_lines():
                                if line:
                                    try:
                                        json_response = json.loads(line)
                                        if "response" in json_response:
                                            full_response += json_response["response"]
                                            response_placeholder.markdown(f"**AI:** {full_response}â–Œ")
                                        if json_response.get("done", False):
                                            break
                                    except json.JSONDecodeError:
                                        continue

                    response_placeholder.markdown(f"**AI:** {full_response}")

                    # AI ì‘ë‹µ ì €ì¥
                    if full_response:
                        st.session_state.chat_history.append({"role": "assistant", "content": full_response})
                    else:
                        st.warning("ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
                    if BACKEND == "vllm":
                        st.info("vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. (http://localhost:8000)")
                    else:
                        st.info("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. (http://localhost:11434)")

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    if st.session_state.chat_history:
        st.divider()
        st.header("ğŸ’­ ëŒ€í™” íˆìŠ¤í† ë¦¬")

        for message in st.session_state.chat_history:
            if message["role"] == "user":
                st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {message['content']}")
            else:
                st.markdown(f"**ğŸ¤– AI:** {message['content']}")
            st.divider()

else:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    # ì‚¬ìš© ë°©ë²• ì•ˆë‚´
    st.header("ğŸ“– ì‚¬ìš© ë°©ë²•")

    if BACKEND == "vllm":
        st.markdown("""
        ### Linux GPU í™˜ê²½ (vLLM)

        1. **ì‹¤í–‰**: `docker-compose --profile linux up -d`
        2. **PDF ì—…ë¡œë“œ**: ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ ì—…ë¡œë“œ
        3. **ì§ˆë¬¸ ì…ë ¥**: ì—…ë¡œë“œëœ PDFì— ëŒ€í•´ ì§ˆë¬¸ ì…ë ¥
        4. **ì‘ë‹µ í™•ì¸**: AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±

        ### í•„ìˆ˜ ì‚¬í•­
        - vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤
        - GPU: NVIDIA L40s ì´ìƒ
        - VRAM: 40GB ì´ìƒ ê¶Œì¥
        - CUDA: 12.1 ì´ìƒ
        """)
    else:
        st.markdown("""
        ### macOS M2 í™˜ê²½ (Ollama)

        1. **ì‹¤í–‰**: `docker-compose --profile mac up -d`
        2. **ëª¨ë¸ ì¤€ë¹„**: ì²« ì‹¤í–‰ì‹œ ìë™ìœ¼ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        3. **PDF ì—…ë¡œë“œ**: ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ ì—…ë¡œë“œ
        4. **ì§ˆë¬¸ ì…ë ¥**: ì—…ë¡œë“œëœ PDFì— ëŒ€í•´ ì§ˆë¬¸ ì…ë ¥
        5. **ì‘ë‹µ í™•ì¸**: AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±

        ### í•„ìˆ˜ ì‚¬í•­
        - Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤
        - Apple Silicon (M1/M2/M3) Mac
        - RAM: 16GB ì´ìƒ ê¶Œì¥
        """)

# í‘¸í„°
st.divider()
st.caption(f"Powered by {backend_name} | Model: {MODEL_NAME}")
